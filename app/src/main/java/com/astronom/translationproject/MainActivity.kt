package com.astronom.translationproject

import android.Manifest
import android.content.Context
import android.content.pm.PackageManager
import android.os.Bundle
import android.util.Log
import androidx.activity.ComponentActivity
import androidx.activity.compose.setContent
import androidx.activity.enableEdgeToEdge
import androidx.camera.core.CameraSelector
import androidx.camera.core.ExperimentalGetImage
import androidx.camera.core.ImageAnalysis
import androidx.camera.core.ImageProxy
import androidx.camera.lifecycle.ProcessCameraProvider
import androidx.camera.view.PreviewView
import androidx.compose.foundation.background
import androidx.compose.foundation.layout.Arrangement
import androidx.compose.foundation.layout.Column
import androidx.compose.foundation.layout.Row
import androidx.compose.foundation.layout.Spacer
import androidx.compose.foundation.layout.fillMaxSize
import androidx.compose.foundation.layout.fillMaxWidth
import androidx.compose.foundation.layout.height
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.layout.size
import androidx.compose.foundation.shape.RoundedCornerShape
import androidx.compose.material3.Button
import androidx.compose.material3.Text
import androidx.compose.runtime.Composable
import androidx.compose.runtime.LaunchedEffect
import androidx.compose.runtime.getValue
import androidx.compose.runtime.mutableStateOf
import androidx.compose.runtime.remember
import androidx.compose.runtime.setValue
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.graphics.vector.ImageVector
import androidx.compose.ui.platform.LocalContext
import androidx.compose.ui.unit.dp
import androidx.compose.ui.unit.sp
import androidx.compose.ui.viewinterop.AndroidView
import androidx.core.app.ActivityCompat
import androidx.core.content.ContextCompat
import androidx.lifecycle.LifecycleOwner
import androidx.lifecycle.compose.LocalLifecycleOwner
import com.astronom.translationproject.ui.theme.TranslationProjectTheme
import com.google.mlkit.common.model.DownloadConditions
import com.google.mlkit.nl.translate.TranslateLanguage
import com.google.mlkit.nl.translate.Translation
import com.google.mlkit.nl.translate.TranslatorOptions
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.text.TextRecognition
import com.google.mlkit.vision.text.latin.TextRecognizerOptions
import java.util.concurrent.Executors
import androidx.camera.core.Preview as CameraPreview


class MainActivity : ComponentActivity() {
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        enableEdgeToEdge()
        if (ContextCompat.checkSelfPermission(this, Manifest.permission.CAMERA)
            != PackageManager.PERMISSION_GRANTED
        ) {
            ActivityCompat.requestPermissions(
                this,
                arrayOf(Manifest.permission.CAMERA),
                0
            )
        }

        setContent {
            TranslationProjectTheme {
                MainScreen()
            }
        }
    }
}

// MainScreen() ë°–ì— ìœ„ì¹˜ (ìµœìƒìœ„ ë ˆë²¨)
val recognizedTextState = mutableStateOf("")
val cameraExecutor = Executors.newSingleThreadExecutor()
val recognizer = TextRecognition.getClient(TextRecognizerOptions.DEFAULT_OPTIONS)

// ë§ˆì§€ë§‰ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•œ ì‹œê°„ì„ ì €ì¥í•  ë³€ìˆ˜
private var lastAnalyzedTimestamp = 0L
private const val ANALYSIS_INTERVAL_MILLIS = 3000L // 3ì´ˆ (3000 ë°€ë¦¬ì´ˆ)


@Composable
fun MainScreen() {

    val context = LocalContext.current

    // recognizedTextStateë¥¼ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.
    val lifecycleOwner = LocalLifecycleOwner.current
    var flag by remember { mutableStateOf(false) }

    // recognizeTextState ê°’ì„ ê°€ì ¸ì™€ UIì— ë°”ì¸ë”©

    val recognizedText by recognizedTextState

    val imageAnalyzer = remember {
        ImageAnalysis.Builder()
            // ë¶„ì„ ì†ë„ë³´ë‹¤ í”„ë ˆì„ ì†ë„ê°€ ë¹ ë¥¼ ë•Œ ìµœì‹  ì´ë¯¸ì§€ë§Œ ìœ ì§€
            .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)
            .build()
            .also {
                // ìˆ˜ì •ëœ YourImageAnalyzer ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
                it.setAnalyzer(cameraExecutor, YourImageAnalyzer())
            }
    }
    var isReady by remember { mutableStateOf(false) }

    val enKoTranslator = remember {
        val options = TranslatorOptions.Builder()
            .setSourceLanguage(TranslateLanguage.ENGLISH)
            .setTargetLanguage(TranslateLanguage.KOREAN)
            .build()
        Translation.getClient(options)
    }

    LaunchedEffect(enKoTranslator) {
        var conditions =
            DownloadConditions.Builder()
//        .requireWifi()
                .build()

        enKoTranslator.downloadModelIfNeeded(conditions)
            .addOnSuccessListener {
                isReady = true
                // Model downloaded successfully. Okay to start translating.
                // (Set a flag, unhide the translation UI, etc.)
            }
            .addOnFailureListener { exception ->
                // Model couldnâ€™t be downloaded or other internal error.
                // ...
            }
    }
    var translated by remember { mutableStateOf("") }
    LaunchedEffect(recognizedText, flag, isReady) { // recognizedText, flag, isReadyê°€ ë³€ê²½ë  ë•Œ ì‹¤í–‰
        if (flag && isReady && recognizedText.isNotEmpty()) { // flagê°€ trueì´ê³ , ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆê³ , ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ìˆì„ ë•Œë§Œ ë²ˆì—­
            enKoTranslator.translate(recognizedText)
                .addOnSuccessListener { translatedText ->
                    translated = translatedText
                }
                .addOnFailureListener { exception ->
                    Log.e("Translation", "Translation failed", exception)
                    translated = "ë²ˆì—­ ì‹¤íŒ¨" // ì‚¬ìš©ìì—ê²Œ ì˜¤ë¥˜ í‘œì‹œ
                }
        } else if (!flag || recognizedText.isEmpty()) {
            translated = "" // ë²ˆì—­ ë¹„í™œì„±í™” ë˜ëŠ” ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
        }
    }
    val resultText = translated.replace(Regex("[a-zA-Z]"), "")


    Column(
        Modifier
            .fillMaxSize()
            .background(Color.Black),
        Arrangement.Top,
        Alignment.CenterHorizontally,

        ) {
        Spacer(Modifier.size(50.dp))
        // ì¹´ë©”ë¼ í”„ë¦¬ë·° ë·° ì¶”ê°€
        CameraPreviewView(
            modifier = Modifier // ğŸš© Modifier ì ìš©
                .fillMaxWidth() // ê°€ë¡œë¥¼ ìµœëŒ€ë¡œ ì±„ìš°ê³ 
                .height(400.dp) // ë†’ì´ë¥¼ 400dpë¡œ ì œí•œ (ì˜ˆì‹œ)
                // .size(300.dp) // ë˜ëŠ” ê°€ë¡œì„¸ë¡œ 300dp ì •ì‚¬ê°í˜•ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
                // .aspectRatio(3f / 4f) // íŠ¹ì • ë¹„ìœ¨ ìœ ì§€ (ì˜ˆ: 3:4)
                .clip(RoundedCornerShape(8.dp)), // ë‘¥ê·¼ ëª¨ì„œë¦¬ ì¶”ê°€ (ì„ íƒ ì‚¬í•­)
            context = context,
            lifecycleOwner = lifecycleOwner,
            imageAnalyzer = imageAnalyzer
        )
        Spacer(Modifier.size(20.dp))
        Button(onClick = {
            flag = !flag
        }) {
            Text(text = "ë²ˆì—­ on/off")
        }
        Spacer(Modifier.size(20.dp))
        Row {
            Text(text = "ì¸ì‹ëœ í…ìŠ¤íŠ¸", fontSize = 25.sp, color = Color.Cyan)
            Spacer(Modifier.size(10.dp))
            Text(text = "ë²ˆì—­ëœ í…ìŠ¤íŠ¸", fontSize = 25.sp, color = Color.Magenta)
        }
        Row(
            Modifier
                .fillMaxWidth()
                .background(Color.LightGray)
        ) {
            if (flag) {

                Text(
                    text = recognizedText,
                    fontSize = 10.sp,
                    lineHeight = 12.sp,
                    modifier = Modifier
                        .weight(1f)
                        .background(Color.Cyan)

                )

                Text(
                    text = resultText,
                    fontSize = 10.sp,
                    lineHeight = 12.sp,
                    modifier = Modifier
                        .weight(1f)
                        .background(Color.Magenta)
                )
            }
        }

    }
}


@Composable
fun CameraPreviewView(
    modifier: Modifier = Modifier, // ğŸš© ì—¬ê¸°ì— Modifier ì¸ì ì¶”ê°€
    context: Context,
    lifecycleOwner: LifecycleOwner,
    imageAnalyzer: ImageAnalysis, // ImageAnalysis ìœ ìŠ¤ì¼€ì´ìŠ¤ë¥¼ ì¸ìë¡œ ë°›ë„ë¡ ì¶”ê°€
) {
    val previewView = remember { PreviewView(context) }

    AndroidView(
        factory = { previewView },
        modifier = modifier
    ) {
        val cameraProviderFuture = ProcessCameraProvider.getInstance(context)
        cameraProviderFuture.addListener({
            val cameraProvider = cameraProviderFuture.get()
            val preview = CameraPreview.Builder().build().also {
                it.setSurfaceProvider(previewView.surfaceProvider)
            }

            val cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA

            try {
                cameraProvider.unbindAll()
                // previewì™€ imageAnalyzerë¥¼ í•¨ê»˜ ë°”ì¸ë”©
                cameraProvider.bindToLifecycle(

                    lifecycleOwner,
                    cameraSelector,
                    preview,
                    imageAnalyzer // ì—¬ê¸°ì— ImageAnalysis ìœ ìŠ¤ì¼€ì´ìŠ¤ ì¶”ê°€
                )
            } catch (e: Exception) {
                e.printStackTrace()
                Log.e("CameraX", "Error binding camera use cases", e)
            }
        }, ContextCompat.getMainExecutor(context))
    }
}


@androidx.annotation.OptIn(ExperimentalGetImage::class)
private class YourImageAnalyzer : ImageAnalysis.Analyzer {
    override fun analyze(imageProxy: ImageProxy) {
        val currentTime = System.currentTimeMillis()

        // 3ì´ˆ (3000ms)ê°€ ì§€ë‚¬ì„ ë•Œë§Œ ì´ë¯¸ì§€ ë¶„ì„ì„ ìˆ˜í–‰
        if (currentTime - lastAnalyzedTimestamp >= ANALYSIS_INTERVAL_MILLIS) {
            val mediaImage = imageProxy.image
            if (mediaImage != null) {
                val image =
                    InputImage.fromMediaImage(mediaImage, imageProxy.imageInfo.rotationDegrees)

                recognizer.process(image)
                    .addOnSuccessListener { visionText ->
                        recognizedTextState.value = visionText.text
                        // Logcatìœ¼ë¡œ í™•ì¸
                        Log.d("ImageAnalysis", "Text recognized: ${visionText.text}")
                    }
                    .addOnFailureListener { e ->
                        Log.e("ImageAnalysis", "Text recognition failed: ${e.message}", e)
                    }
                    .addOnCompleteListener {
                        // ML Kit ì²˜ë¦¬ê°€ ì™„ë£Œëœ í›„ íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸ ë° ImageProxy ë‹«ê¸°
                        lastAnalyzedTimestamp = currentTime // ì—¬ê¸°ì„œ ì—…ë°ì´íŠ¸í•´ì•¼, ì²˜ë¦¬ê°€ ì™„ë£Œëœ í›„ ë‹¤ìŒ ê°„ê²©ì´ ì‹œì‘ë¨
                        imageProxy.close()
                    }
            } else {
                imageProxy.close() // mediaImageê°€ nullì¸ ê²½ìš°ì—ë„ ë‹«ì•„ì¤˜ì•¼ í•¨
            }
        } else {
            // 3ì´ˆ ê°„ê²©ì´ ì•„ì§ ì•ˆ ì§€ë‚¬ìœ¼ë©´ ì´ë¯¸ì§€ ë¶„ì„ì„ ê±´ë„ˆë›°ê³  ë°”ë¡œ ë‹«ìŒ
            imageProxy.close()
        }
    }
}